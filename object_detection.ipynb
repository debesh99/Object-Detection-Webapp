{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet('yolov3.weights' , 'resources\\cfg\\yolov3.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "with open('resources\\data\\coco.names','r') as f:\n",
    "    classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Images\\img3.jpg')\n",
    "height,width,_ = img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing of Image in accordance with yolov3\n",
    "blob = cv2.dnn.blobFromImage(img , 1/255 , (416,416) , (0,0,0) , swapRB = True , crop=False)\n",
    "# Here 1/255 means normalize the image by diving it's pixels by 255\n",
    "# 416,416 is the dimensions of image as yolo supports square size images\n",
    "# swapRB : OpenCV assumes images are in BGR channel order; however, the `mean` value assumes we are using RGB order. To resolve this discrepancy we can swap the R and B channels in image by setting this value to `True`\n",
    "#Here 0,0,0  These are our mean subtraction values. They can be a 3-tuple of the RGB means or they can be a single value in which case the supplied value is subtracted from every channel of the image.\n",
    "\n",
    "# To implement blob\n",
    "# for b in blob:\n",
    "#     for n ,img_blob in enumerate(b):\n",
    "#         cv2.imshow(str(n) , img_blob)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \n",
    "net.setInput(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "layerOutputs = net.forward(output_layers_names)\n",
    "\n",
    "\n",
    "# net.getUnconnectedOutLayers() gives the position of the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process starts to identifying objects un output in boxes\n",
    "boxes = []\n",
    "confidence_s = []\n",
    "class_id_s =  []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First for loop is used to extract all the information from the layer outputs and then second for loop is used to extract information from each of the output (detection)\n",
    "\n",
    "for output in layerOutputs:\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence>0.5:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y  = int( detection[1] * height)\n",
    "            w = int( detection[2] * width)\n",
    "            h = int( detection[3] * height)\n",
    "\n",
    "            x = int(center_x - w/2)\n",
    "            y = int(center_x - h/2)\n",
    "            # Positions of the upperleft corner(starting corner)\n",
    "\n",
    "            boxes.append([x,y,w,h])\n",
    "            confidence_s.append((float(confidence)))\n",
    "            class_id_s.append(class_id)\n",
    "# For each of the detection(box) it contains 85 parameters\n",
    "# 80 : objects\n",
    "# FIrst 5 parameters\n",
    "# 4  : Location of bounding box (X,Y,W,H)\n",
    "        # X,Y - CENTER COORDINATES( detection[0],detection[1])\n",
    "        # W, H - WIDTH AND HEIGHT( detection[2],detection[3])\n",
    "# 1 : confidence of box\n",
    "# so detection starts after 5 till the end\n",
    "\n",
    "#   np.argmax(scores) to locate highest scores location\n",
    "# here width and height multiplied from the yolo value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "[ 6  5 16 28 18  0 23  2  9 11]\n"
     ]
    }
   ],
   "source": [
    "# When we pwerform object detection it happens that we have one more than one boxes or  the same objects so we will use non maximum suppressions(NMS) to only keep their higher scores boxes. we need to pass 4 parameter to the function that contains all the boxes  , their corresponding confidence\n",
    "\n",
    "print(len(boxes))\n",
    "indexes = cv2.dnn.NMSBoxes(boxes , confidence_s , 0.5 , 0.4)\n",
    "print(indexes.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonts\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "colors = np.random.uniform(0,255,size=(len(boxes),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of all information from the code to dislay the output\n",
    "for i in indexes.flatten():\n",
    "    x,y,w,h = boxes[i]\n",
    "    label = str(classes[class_id_s[i]])\n",
    "    confidence = str(round(confidence_s[i],2))\n",
    "    color = colors[i]\n",
    "    cv2.rectangle(img , (x,y) , (x+w , y+h) , color , 2)\n",
    "    cv2.putText(img,label + \" \"+ confidence , (x , y+20) , font , 2 , (0,0,0) ,2)\n",
    "    # (function) putText: (img, text, org, fontFace, fontScale, color, thickness=..., lineType=..., bottomLeftOrigin=...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show image\n",
    "cv2.imshow('Image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b6e466f30f384edd812a1784ad6cc9342ecf1c8cee5293f65f51494767d012a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
